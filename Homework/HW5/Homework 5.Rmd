---
title: "Homework 5"
author: "Gianni Spiga"
date: "5/21/2022"
output: html_document
---

## P1 
8.1.) 

```{r}
Sigma <- matrix(c(5, 2, 2, 2), nrow = 2, ncol = 2 )
Sigma

eigSigma = eigen(Sigma)
eigSigma$values
eigSigma$vectors
```

The population principal components are:
$$
Y_1 = -0.8944X_1 + 0.4472X_2\\
Y_2 = -0.4472X_1 - 0.8944X_2\\

$$
The proportion of total variance explained by $Y_1$ is:
```{r}
eigSigma$values[1] / sum(eigSigma$values)
```
8.2.)
a.)
```{r}
D = diag(sqrt(diag(Sigma)))
D
corr = solve(D) %*% Sigma %*% solve(D)
corr
eigCorr = eigen(corr)
eigCorr$values
eigCorr$vectors
```
The population principal components are:
$$
Y_1 = -0.7071X_1 + 0.7071X_2\\
Y_2 = -0.7071X_1 - 0.7071X_2\\
$$
The proportion of total variance explained by $Y_1$ is:
```{r}
eigCorr$values[1] / sum(eigCorr$values)
```
b.)
As we can see above, the estiamtes are not the same. When we standardize our variables, the eigenvectors become identical. Since we have now controlled each eigenvector to have the same mean and variance, they contribute equally, unlike before. 
c.)
```{r}
#PDF Pg 460
rho1 <- eigCorr$vectors[1,1] * sqrt(eigCorr$values[1])
rho1
rho12 <- eigCorr$vectors[1,2] * sqrt(eigCorr$values[1])
rho12
rho2 <- eigCorr$vectors[2,2] * sqrt(eigCorr$values[2])
rho2
```
## P2
8.10
a.) and b.) 
```{r}
stock <- read.table("~/GitHub/STA135/Homework/HW5/T8-4.dat")
names(stock) <- c("JP Morgan", "Citibank", "Wells Fargo", "Royal Dutch Shell", "Exxon Mobil")
stock

cov(stock)

#Using spectral decomposition instead of SVD
princomp(stock)$sdev ^ 2
sum(princomp(stock)$sdev ^ 2)
summary(princomp(stock))
```
As we can see from above, the cumulative proportion of variance explained by the first 3 variables is 0.899 percent. 
c.)
```{r}
#pdf pg 456-457
lambda1_lower <- princomp(stock)$sdev[1]^2 / (1+  qnorm(0.1/(2*3), lower.tail = FALSE) * sqrt(2/ nrow(stock)))
lambda1_upper <- princomp(stock)$sdev[1]^2 / (1 -  qnorm(0.1/(2*3), lower.tail = FALSE) * sqrt(2/ nrow(stock)))
c(lambda1_lower, lambda1_upper)

lambda2_lower <- princomp(stock)$sdev[2]^2 / (1+  qnorm(0.1/(2*3), lower.tail = FALSE) * sqrt(2/ nrow(stock)))
lambda2_upper <- princomp(stock)$sdev[2]^2 / (1-  qnorm(0.1/(2*3), lower.tail = FALSE) * sqrt(2/ nrow(stock)))
c(lambda2_lower, lambda2_upper)

lambda3_lower <- princomp(stock)$sdev[3]^2 / (1+  qnorm(0.1/(2*3), lower.tail = FALSE) * sqrt(2/ nrow(stock)))
lambda3_upper <- princomp(stock)$sdev[3]^2 / (1 -  qnorm(0.1/(2*3), lower.tail = FALSE) * sqrt(2/ nrow(stock)))
c(lambda3_lower, lambda3_upper)
```
d.) Given the high cumulative proportion of variance explained by the first 2 variables, it would be logical to conclude that we could summarize stock prices in two dimensions. 

## P3
8.22.)

```{r}
bulls <- read.table("~/GitHub/STA135/Homework/HW5/T1-10.dat")
names(bulls) <- c("Breed", "SalePr", "YrHgt", "FtFrBody", "PrctFFB", "Frame", "BkFat", "SaleHt", "SaleWt")
breed <- bulls[,1]
bulls <- bulls[,-c(1, 2)]
bulls

pca.bulls <- princomp(bulls)

summary(princomp(bulls))

plot(pca.bulls)
```

By looking at the cumulative proportion of our components, we can logically conclude that we only need at most two components, which explain over 99% of the variability in the data.

b.) We can think of the first component as one regarding the size of a bull, where we have information about the body fat, frame, etc. The other component we can think to summaraize the information regarding the bull at time of sale, in this case the sale height and weight. 

c.)We can use the first principal component to  create a body size index. 

d.)
```{r}
# http://www.sthda.com/english/wiki/ggfortify-extension-to-ggplot2-to-handle-some-popular-packages-r-software-and-data-visualization
library(ggfortify)
bulls$Breed <- breed
plot <- autoplot(pca.bulls, loadings = TRUE, loadings.label = TRUE, data = bulls, colour = "Breed") + labs(colour = breed)
#plot
library(plotly)
ggplotly(plot)
```

With the plot we can see that there is no clear clustering forming for breeds, most overlap. One notable outlier is in the top right of our scatter plot, with high values in both components, coming from Breed 8. 

e.)
```{r}
library(car)
qqPlot(prcomp(bulls)$x[,1])
```

We can see here that the residuals for the first PCA component from our data is approximately normal. 

## P4
9.1.) 
```{r}
rho <- matrix(c(1, .63, .45, .63, 1, .35, .45, .35, 1), nrow = 3, ncol = 3)
psi <- matrix(c(.19, 0, 0, 0, .51, 0, 0,0, .75), nrow = 3, ncol = 3)
L = c(.9, .7, .5)
L %*% t(L) + psi
#Confirm that they are equal
setequal(rho, L %*% t(L) + psi)
```
9.2.)
a.)
```{r}
h2 <- L^2
h2
```
b.)

By equation 9.5, we know that the $Cov(X_i , F_j) = l_{ij}$, in this case $[.9, .7, .5]$ Z1 carries the greatest weight on F1, since it has the largest correlation. 

hi